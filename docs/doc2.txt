Retrieval-Augmented Generation (RAG)

Retrieval-Augmented Generation (RAG) is an AI technique that combines information retrieval with text generation. It enhances the capabilities of large language models by allowing them to access external knowledge sources during the generation process.

RAG works in two main phases:
1. Retrieval Phase - The system searches through a knowledge base to find relevant documents or passages
2. Generation Phase - The retrieved information is used as context for the language model to generate accurate responses

Benefits of RAG include:
- Access to up-to-date information
- Reduced hallucination in AI responses
- Better domain-specific knowledge
- Improved accuracy for factual questions

Common RAG implementations use vector databases like FAISS, Pinecone, or Weaviate to store and search through document embeddings. The process typically involves:
- Document chunking and embedding
- Vector similarity search
- Context injection into prompts
- Response generation with retrieved context

RAG is particularly useful for question-answering systems, chatbots, and applications requiring factual accuracy.


Practical RAG Implementation Notes

- Chunking Strategy: Split long documents into overlapping passages (e.g., 300–800 tokens, 10–15% overlap) to improve retrieval granularity.
- Embedding Consistency: Use the same embedding model for both documents and queries (e.g., "gemini-embedding-001").
- Similarity Metric: Normalize vectors and use inner product to approximate cosine similarity in FAISS.
- Prompt Construction: Keep prompts concise; include retrieved snippets with clear separators and source labels for citation.
- Index Maintenance: Rebuild or incrementally update the index when the underlying documents change.

Common Pitfalls

- Overly Large Chunks: May dilute relevance and hurt retrieval quality.
- Missing Normalization: Can degrade cosine similarity performance when using inner product.
- No Source Control: Answers may appear ungrounded if prompts omit file names or spans.
- Over-Retrieval: Too many contexts can confuse the generator; tune k (e.g., 3–5).

Evaluation Hints

- Track retrieval metrics (recall@k) and manually inspect top results.
- Assess faithfulness: verify that generated answers do not contradict the retrieved context.
- Monitor latency and cost; cache embeddings and reuse indices where possible.